{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daurice/Deep-Learning--RNN/blob/main/LSTM_movie_review_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a1b7b09",
      "metadata": {
        "id": "1a1b7b09"
      },
      "source": [
        "## Movie Review Prediction RNN Model\n",
        "\n",
        "Dataset- imdb dataset present in keras library in Python\n",
        "The IMDB dataset in the Keras library is a popular dataset used for sentiment analysis.\n",
        "It consists of 25,000 movie reviews from IMDB, labeled as either positive or negative.\n",
        "\n",
        "For convenience, words are indexed by overall frequency in the dataset,\n",
        "for instance the integer \"3\" encodes the 3rd most frequent word in the data.\n",
        "\n",
        "This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\"."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5xLNbC1XmzHq"
      },
      "id": "5xLNbC1XmzHq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "73b0a499",
      "metadata": {
        "id": "73b0a499"
      },
      "source": [
        "<h3>Word Indexing by Frequency</h3>\n",
        "\n",
        "In the IMDB dataset, words are indexed based on their frequency of occurrence.\n",
        "\n",
        "This means that:\n",
        "\n",
        "The most frequent word in the dataset is assigned the index 1.\n",
        "The second most frequent word is assigned the index 2.\n",
        "The third most frequent word is assigned the index 3, and so on.\n",
        "\n",
        "\n",
        "Example\n",
        "Let’s say we have a small dataset with the following word frequencies:\n",
        "\n",
        "“the” appears 5000 times\n",
        "\n",
        "“movie” appears 3000 times\n",
        "\n",
        "“was” appears 2000 times\n",
        "\n",
        "“great” appears 1500 times\n",
        "\n",
        "“bad” appears 1000 times\n",
        "\n",
        "\n",
        "The indexing would look like this:\n",
        "\n",
        "“the” -> 1\n",
        "“movie” -> 2\n",
        "“was” -> 3\n",
        "“great” -> 4\n",
        "“bad” -> 5\n",
        "\n",
        "\n",
        "Filtering Operations\n",
        "\n",
        "This indexing allows for quick filtering operations. For example:\n",
        "\n",
        "---Top 10,000 Most Common Words: You can limit your dataset to only include the top 10,000 most frequent words.\n",
        "This helps in reducing the vocabulary size and focusing on the most relevant words.\n",
        "\n",
        "---Eliminate the Top 20 Most Common Words: You can exclude the top 20 most frequent words, which are often stop words like “the,” “is,” “and,” etc., that might not add significant value to the analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b06201a9",
      "metadata": {
        "id": "b06201a9"
      },
      "source": [
        "https://keras.io/api/datasets/imdb/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7c6c323",
      "metadata": {
        "id": "b7c6c323"
      },
      "source": [
        "## Import Required Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f3838fb",
      "metadata": {
        "id": "9f3838fb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding,LSTM\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from keras.utils import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db6498e0",
      "metadata": {
        "id": "db6498e0"
      },
      "source": [
        "## Import Dataset for each sample load 20000 most frequently occuring words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4797f24a",
      "metadata": {
        "id": "4797f24a"
      },
      "outputs": [],
      "source": [
        "# load imdb data https://keras.io/api/datasets/imdb/\n",
        "(X_train,y_train),(X_test,y_test) = imdb.load_data(num_words=20000)\n",
        "#load first 20000 most frequent occuring words\n",
        "#imdb data is already split in the train and test data by default"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2359f24",
      "metadata": {
        "id": "c2359f24"
      },
      "source": [
        "## Explore Imported Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58901209",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58901209",
        "outputId": "a6f27f66-e7eb-4030-f83d-1e4f7b3f0251"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Check the shape of the training data.\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XQVVIDFCpRbG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQVVIDFCpRbG",
        "outputId": "d35dd4f6-7791-4874-f071-9804a5067362"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Checkig the shape of the testing data\n",
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02ba1ae5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02ba1ae5",
        "outputId": "7851c96b-ec61-4140-909c-6f26546b936b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1,\n",
              " 194,\n",
              " 1153,\n",
              " 194,\n",
              " 8255,\n",
              " 78,\n",
              " 228,\n",
              " 5,\n",
              " 6,\n",
              " 1463,\n",
              " 4369,\n",
              " 5012,\n",
              " 134,\n",
              " 26,\n",
              " 4,\n",
              " 715,\n",
              " 8,\n",
              " 118,\n",
              " 1634,\n",
              " 14,\n",
              " 394,\n",
              " 20,\n",
              " 13,\n",
              " 119,\n",
              " 954,\n",
              " 189,\n",
              " 102,\n",
              " 5,\n",
              " 207,\n",
              " 110,\n",
              " 3103,\n",
              " 21,\n",
              " 14,\n",
              " 69,\n",
              " 188,\n",
              " 8,\n",
              " 30,\n",
              " 23,\n",
              " 7,\n",
              " 4,\n",
              " 249,\n",
              " 126,\n",
              " 93,\n",
              " 4,\n",
              " 114,\n",
              " 9,\n",
              " 2300,\n",
              " 1523,\n",
              " 5,\n",
              " 647,\n",
              " 4,\n",
              " 116,\n",
              " 9,\n",
              " 35,\n",
              " 8163,\n",
              " 4,\n",
              " 229,\n",
              " 9,\n",
              " 340,\n",
              " 1322,\n",
              " 4,\n",
              " 118,\n",
              " 9,\n",
              " 4,\n",
              " 130,\n",
              " 4901,\n",
              " 19,\n",
              " 4,\n",
              " 1002,\n",
              " 5,\n",
              " 89,\n",
              " 29,\n",
              " 952,\n",
              " 46,\n",
              " 37,\n",
              " 4,\n",
              " 455,\n",
              " 9,\n",
              " 45,\n",
              " 43,\n",
              " 38,\n",
              " 1543,\n",
              " 1905,\n",
              " 398,\n",
              " 4,\n",
              " 1649,\n",
              " 26,\n",
              " 6853,\n",
              " 5,\n",
              " 163,\n",
              " 11,\n",
              " 3215,\n",
              " 10156,\n",
              " 4,\n",
              " 1153,\n",
              " 9,\n",
              " 194,\n",
              " 775,\n",
              " 7,\n",
              " 8255,\n",
              " 11596,\n",
              " 349,\n",
              " 2637,\n",
              " 148,\n",
              " 605,\n",
              " 15358,\n",
              " 8003,\n",
              " 15,\n",
              " 123,\n",
              " 125,\n",
              " 68,\n",
              " 2,\n",
              " 6853,\n",
              " 15,\n",
              " 349,\n",
              " 165,\n",
              " 4362,\n",
              " 98,\n",
              " 5,\n",
              " 4,\n",
              " 228,\n",
              " 9,\n",
              " 43,\n",
              " 2,\n",
              " 1157,\n",
              " 15,\n",
              " 299,\n",
              " 120,\n",
              " 5,\n",
              " 120,\n",
              " 174,\n",
              " 11,\n",
              " 220,\n",
              " 175,\n",
              " 136,\n",
              " 50,\n",
              " 9,\n",
              " 4373,\n",
              " 228,\n",
              " 8255,\n",
              " 5,\n",
              " 2,\n",
              " 656,\n",
              " 245,\n",
              " 2350,\n",
              " 5,\n",
              " 4,\n",
              " 9837,\n",
              " 131,\n",
              " 152,\n",
              " 491,\n",
              " 18,\n",
              " 2,\n",
              " 32,\n",
              " 7464,\n",
              " 1212,\n",
              " 14,\n",
              " 9,\n",
              " 6,\n",
              " 371,\n",
              " 78,\n",
              " 22,\n",
              " 625,\n",
              " 64,\n",
              " 1382,\n",
              " 9,\n",
              " 8,\n",
              " 168,\n",
              " 145,\n",
              " 23,\n",
              " 4,\n",
              " 1690,\n",
              " 15,\n",
              " 16,\n",
              " 4,\n",
              " 1355,\n",
              " 5,\n",
              " 28,\n",
              " 6,\n",
              " 52,\n",
              " 154,\n",
              " 462,\n",
              " 33,\n",
              " 89,\n",
              " 78,\n",
              " 285,\n",
              " 16,\n",
              " 145,\n",
              " 95]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Print a sample review (as word indices) and its label.\n",
        "#printing the word with indices 1\n",
        "X_train[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68c6659e",
      "metadata": {
        "id": "68c6659e"
      },
      "source": [
        "Understanding the Output\n",
        "Word Indices: Each number in the list is an index that maps to a word in the dataset’s vocabulary.\n",
        "\n",
        "Sequence: The sequence of numbers represents the order of words in the review.\n",
        "\n",
        "Example Breakdown\n",
        "Let’s take a closer look at the first few indices:\n",
        "\n",
        "1: This is typically a special token used to mark the start of a sequence.\n",
        "\n",
        "194, 1153, 194: These numbers correspond to specific words in the vocabulary.\n",
        "For example, if 194 maps to the word “movie,” then every occurrence of 194 in the sequence represents the word “movie.”"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae79d3c8",
      "metadata": {
        "id": "ae79d3c8"
      },
      "source": [
        "<h3>Dataset vocabulary</h3>\n",
        "A dataset vocabulary refers to the set of unique words or tokens that appear in a dataset.\n",
        "\n",
        "\n",
        "\n",
        "Key Points about Dataset Vocabulary\n",
        "\n",
        "---Unique Words: The vocabulary consists of all the unique words present in the dataset. For example, if your dataset contains the sentences “I love movies” and “I love coding,” the vocabulary would be {\"I\", \"love\", \"movies\", \"coding\"}.\n",
        "\n",
        "---Word Indexing: Each word in the vocabulary is often assigned a unique index.\n",
        "This indexing helps in converting words into numerical representations that can be processed by machine learning models.\n",
        "For example, {\"I\": 1, \"love\": 2, \"movies\": 3, \"coding\": 4}.\n",
        "\n",
        "---Frequency-Based Filtering: In some cases, the vocabulary is limited to the most frequent words.\n",
        "For instance, you might only keep the top 10,000 most frequent words in the dataset to reduce complexity and focus on the most relevant words.\n",
        "\n",
        "---Handling Out-of-Vocabulary Words: Words that are not in the vocabulary are often replaced with a special token\n",
        "(e.g., <OOV> or 0). This helps in managing words that the model has not seen during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adb791cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adb791cf",
        "outputId": "c63dae55-cf60-49af-fa04-8e20d0a04ebe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 0, 1, 0])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a1d012b",
      "metadata": {
        "id": "0a1d012b"
      },
      "source": [
        "## Add Padding to make inputs of same size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eed8a285",
      "metadata": {
        "id": "eed8a285"
      },
      "source": [
        "The pad_sequences function in Keras is used to ensure that all sequences in a dataset have the same length.\n",
        "\n",
        "This is important because neural networks require inputs of uniform size.\n",
        "\n",
        "The padding='post' argument specifies that padding should be added to the end of each sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d973caa5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "d973caa5",
        "outputId": "407478bf-b2e4-413d-ea9b-90b0f0d81bee"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\npad_sequences: This function pads sequences to the same length.\\npadding=‘post’: This argument specifies that padding should be added to the end (or “post”) of each sequence.\\n\\nExample\\nExplanation\\nOriginal Sequences: [1, 2, 3], [4, 5], [6]\\nPadded Sequences: [1, 2, 3], [4, 5, 0], [6, 0, 0]\\n'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Use pad_sequences to ensure all sequences have the same length.\n",
        "X_train = pad_sequences(X_train,padding='post')\n",
        "X_test = pad_sequences(X_test,padding='post')\n",
        "\n",
        "'''\n",
        "pad_sequences: This function pads sequences to the same length.\n",
        "padding=‘post’: This argument specifies that padding should be added to the end (or “post”) of each sequence.\n",
        "\n",
        "Example\n",
        "Explanation\n",
        "Original Sequences: [1, 2, 3], [4, 5], [6]\n",
        "Padded Sequences: [1, 2, 3], [4, 5, 0], [6, 0, 0]\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OnsPwM1LxEqV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnsPwM1LxEqV",
        "outputId": "278647df-a03b-441d-8a77-3e0cc617b4ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([   1,  194, 1153, ...,    0,    0,    0], dtype=int32)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checking on the padding\n",
        "X_train[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d2fb17f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "1d2fb17f",
        "outputId": "25ce86f1-0854-4b6f-d8c8-c982f9216063"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nkey purposes of the Embedding layer in short bullet points:\\n\\n---Dimensionality Reduction: Converts high-dimensional sparse vectors into dense, lower-dimensional vectors.\\n\\n---Capturing Semantic Relationships: Learns word embeddings that capture the meanings and relationships between words.\\n\\n---Handling Variable Input Lengths: Maps words to fixed-size vectors, making it easier to process sequences of different lengths.\\n\\n---Improving Model Performance: Provides meaningful word representations, enhancing the performance of NLP tasks.\\n\\n'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "key purposes of the Embedding layer in short bullet points:\n",
        "\n",
        "---Dimensionality Reduction: Converts high-dimensional sparse vectors into dense, lower-dimensional vectors.\n",
        "\n",
        "---Capturing Semantic Relationships: Learns word embeddings that capture the meanings and relationships between words.\n",
        "\n",
        "---Handling Variable Input Lengths: Maps words to fixed-size vectors, making it easier to process sequences of different lengths.\n",
        "\n",
        "---Improving Model Performance: Provides meaningful word representations, enhancing the performance of NLP tasks.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e139e3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "5e139e3e",
        "outputId": "a00fd69b-70d2-4f25-8b95-0ff0cadf0939"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nkey purposes of the LSTM (Long Short-Term Memory) layer in short bullet points:\\n\\n---Handling Long-Term Dependencies: Captures and retains information over long sequences, addressing the vanishing gradient problem.\\n\\n---Sequence Learning: Effective for tasks involving sequential data, such as time series prediction, language modeling, and speech recognition.\\n\\n---Memory Cells: Uses memory cells to store information, allowing the network to learn which information to keep or discard.\\n\\n---Gating Mechanisms: Employs input, output, and forget gates to control the flow of information, enhancing the model’s ability to learn complex patterns.\\n'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "key purposes of the LSTM (Long Short-Term Memory) layer in short bullet points:\n",
        "\n",
        "---Handling Long-Term Dependencies: Captures and retains information over long sequences, addressing the vanishing gradient problem.\n",
        "\n",
        "---Sequence Learning: Effective for tasks involving sequential data, such as time series prediction, language modeling, and speech recognition.\n",
        "\n",
        "---Memory Cells: Uses memory cells to store information, allowing the network to learn which information to keep or discard.\n",
        "\n",
        "---Gating Mechanisms: Employs input, output, and forget gates to control the flow of information, enhancing the model’s ability to learn complex patterns.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9063b76",
      "metadata": {
        "id": "b9063b76"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28169f54",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "28169f54",
        "outputId": "2f205fca-2e54-48db-bc9f-fdbcb60a68c4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n1. Initialize a sequential model.\\n\\n2. Add an Embedding layer to convert word indices to dense vectors.\\n\\n3. Add an LSTM layer to capture long-term dependencies.\\n\\n4. Add a Dense layer with a softmax activation function for binary classification.\\n'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "1. Initialize a sequential model.\n",
        "\n",
        "2. Add an Embedding layer to convert word indices to dense vectors.\n",
        "\n",
        "3. Add an LSTM layer to capture long-term dependencies.\n",
        "\n",
        "4. Add a Dense layer with a softmax activation function for binary classification.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be0fd5ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "be0fd5ba",
        "outputId": "474d0948-3ecd-4d29-bd5c-ea3f62c48497"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Initializes a sequential model, which is a linear stack of layers.\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "'''\n",
        "Adds an Embedding layer.\n",
        "20000: Size of the vocabulary (top 20,000 most frequent words).\n",
        "128: Dimension of the dense embedding vectors.\n",
        "\n",
        "Purpose: Converts high-dimensional sparse vectors into dense, lower-dimensional vectors.\n",
        "Parameters: 20000 (vocabulary size), 128 (embedding dimension).\n",
        "'''\n",
        "model.add(Embedding(20000, 128))\n",
        "\n",
        "'''\n",
        "Adds an LSTM (Long Short-Term Memory) layer.\n",
        "128: Number of LSTM units.\n",
        "dropout=0.2: Dropout rate for regularization,\n",
        "which helps prevent overfitting by randomly setting 20% of the input units to 0\n",
        "at each update during training.\n",
        "'''\n",
        "model.add(LSTM(128, dropout=0.2))\n",
        "\n",
        "'''\n",
        "Adds a Dense (fully connected) layer.\n",
        "1: Number of output units (since this is a binary classification problem).\n",
        "activation=‘softmax’: For binary classification, sigmoid is typically used instead of softmax.\n",
        "'''\n",
        "model.add(Dense(1, activation='softmax'))\n",
        "\n",
        "#present summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5e19a81",
      "metadata": {
        "id": "d5e19a81"
      },
      "source": [
        "## Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53080b2b",
      "metadata": {
        "id": "53080b2b"
      },
      "outputs": [],
      "source": [
        "#compling the model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68bf7463",
      "metadata": {
        "id": "68bf7463"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "445eec8b",
      "metadata": {
        "id": "445eec8b"
      },
      "source": [
        "# Caution--- Please run this model only when your system is free for one Epoch it could take you about 1 hour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f7c18ec",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f7c18ec",
        "scrolled": true,
        "outputId": "5ac2641e-bb38-47ea-91a0-975c4a0809fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1429s\u001b[0m 18s/step - accuracy: 0.4980 - loss: 0.6937 - val_accuracy: 0.4938 - val_loss: 0.6932\n",
            "Epoch 2/5\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1430s\u001b[0m 18s/step - accuracy: 0.5019 - loss: 0.6933 - val_accuracy: 0.4938 - val_loss: 0.6936\n",
            "Epoch 3/5\n",
            "\u001b[1m42/79\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m10:47\u001b[0m 18s/step - accuracy: 0.5043 - loss: 0.6933"
          ]
        }
      ],
      "source": [
        "model.fit(X_train, y_train,\n",
        "          batch_size=256,\n",
        "          epochs=5,\n",
        "          verbose=1,\n",
        "          validation_split=0.2)\n",
        "\n",
        "#The following is not an error but result of intrupting the kernel in between as the model was taking about 1 hour for each epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3ee592e",
      "metadata": {
        "id": "c3ee592e"
      },
      "source": [
        "## Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bd16e82",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5bd16e82"
      },
      "outputs": [],
      "source": [
        "acc = model.evaluate(X_test, y_test,\n",
        "                            batch_size=32,\n",
        "                            verbose=1)\n",
        "acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47217381",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "47217381"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}